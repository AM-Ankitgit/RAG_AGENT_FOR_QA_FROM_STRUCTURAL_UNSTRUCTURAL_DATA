{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install chromadb\n",
    "# !pip install unstructured\n",
    "# !pip install tiktoken\n",
    "# !pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/brainwired/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28816,
     "status": "ok",
     "timestamp": 1737778986142,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "Z9o2o7YAiD9U",
    "outputId": "4285ce6a-7275-4c25-8d21-6cc70a8d0f49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/botenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Table, CompositeElement\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Extract raw elements from the PDF\n",
    "# raw_pdf_elements = partition_pdf(\n",
    "#     filename=\"/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/01_pdf_mysql_ai_agent/RAG_AGENT_FOR_QA_FROM_STRUCTURAL_UNSTRUCTURAL_DATA/gst_notebook/PDF_DATA/75183bos60698-p1-m1.pdf\",\n",
    "#     extract_images_in_pdf=False,\n",
    "#     infer_table_structure=True,\n",
    "#     chunking_strategy=\"by_title\",\n",
    "#     max_characters=4000,\n",
    "#     new_after_n_chars=3800,\n",
    "#     combine_text_under_n_chars=2000,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18780/3136690713.py:13: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_function = OpenAIEmbeddings()\n",
      "/tmp/ipykernel_18780/3136690713.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Connection error.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Table, CompositeElement\n",
    "\n",
    "# Directory paths\n",
    "pdf_directory = \"/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/01_pdf_mysql_ai_agent/RAG_AGENT_FOR_QA_FROM_STRUCTURAL_UNSTRUCTURAL_DATA/gst_notebook/PDF_DATA\"  # Directory containing your PDFs\n",
    "chroma_db_directory = \"/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/01_pdf_mysql_ai_agent/RAG_AGENT_FOR_QA_FROM_STRUCTURAL_UNSTRUCTURAL_DATA/gst_notebook/DATA/GST_DATA_VECTOR_DB\"\n",
    "# Initialize ChromaDB with persistence\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"pdf_data\",\n",
    "    persist_directory=chroma_db_directory,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Function to extract and process data from a single PDF\n",
    "def process_pdf(pdf_path):\n",
    "    # print(f\"Processing: {pdf_path}\")\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "    )\n",
    "\n",
    "    # Separate text and table elements\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if isinstance(element, Table):\n",
    "            table_elements.append(str(element))\n",
    "        elif isinstance(element, CompositeElement):\n",
    "            text_elements.append(str(element))\n",
    "    \n",
    "    return text_elements, table_elements\n",
    "\n",
    "# Batch process all PDFs in the directory\n",
    "try:\n",
    "    count=0\n",
    "    all_files = os.listdir(pdf_directory)\n",
    "    for pdf_file in all_files:\n",
    "        # print(pdf_file)\n",
    "        count+=1\n",
    "        \n",
    "        # print(pdf_file)\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            text_elements, table_elements = process_pdf(pdf_path)\n",
    "            # print(text_elements,table_elements)\n",
    "\n",
    "            # Generate unique IDs for each document\n",
    "            text_doc_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "            table_doc_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "\n",
    "            # Prepare text and table documents\n",
    "            text_documents = [\n",
    "                Document(page_content=text, metadata={\"type\": \"text\", \"source\": pdf_file, \"doc_id\": doc_id})\n",
    "                for text, doc_id in zip(text_elements, text_doc_ids)\n",
    "            ]\n",
    "            table_documents = [\n",
    "                Document(page_content=table, metadata={\"type\": \"table\", \"source\": pdf_file, \"doc_id\": doc_id})\n",
    "                for table, doc_id in zip(table_elements, table_doc_ids)\n",
    "            ]\n",
    "\n",
    "            # Add documents to ChromaDB\n",
    "            # print(table_documents)\n",
    "            if text_documents:\n",
    "                vectorstore.add_documents(text_documents)\n",
    "            if table_documents:\n",
    "                print(table_documents)\n",
    "                vectorstore.add_documents(table_documents)\n",
    "            print(count)\n",
    "    # Persist the data to disk\n",
    "    vectorstore.persist()\n",
    "    print(\"All PDFs have been processed and stored in ChromaDB.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Webscrappig Tool\n",
    "\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "\n",
    "\n",
    "# def crawl_site_for_pdfs(base_url, visited=set()):\n",
    "#     \"\"\"Recursively crawl the site to find all PDF links.\"\"\"\n",
    "#     response = requests.get(base_url)\n",
    "#     if response.status_code != 200:\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     pdf_links = []\n",
    "\n",
    "#     # Find all links\n",
    "#     for link in soup.find_all('a', href=True):\n",
    "#         href = link['href']\n",
    "#         # Check if it's a PDF\n",
    "#         if href.endswith('.pdf'):\n",
    "#             full_url = href if href.startswith('http') else base_url + href\n",
    "#             pdf_links.append(full_url)\n",
    "#         # Crawl other pages on the same domain\n",
    "#         elif base_url in href and href not in visited:\n",
    "#             visited.add(href)\n",
    "#             pdf_links.extend(crawl_site_for_pdfs(href, visited))\n",
    "\n",
    "#     return pdf_links\n",
    "\n",
    "# # Start crawling\n",
    "# all_pdf_links = crawl_site_for_pdfs(\"https://www.icai.org/post/19142\")\n",
    "# print(f\"Total PDFs found: {len(all_pdf_links)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# # List of PDF URLs (obtained from the previous step)\n",
    "# all_pdf_links\n",
    "\n",
    "# # Load each PDF using WebBaseLoader\n",
    "# documents = []\n",
    "# for pdf_url in all_pdf_links[0:1]:\n",
    "#     loader = WebBaseLoader(pdf_url)\n",
    "#     documents.extend(loader.load())\n",
    "\n",
    "# # Display content of the loaded documents\n",
    "# for i, doc in enumerate(documents):\n",
    "#     print(f\"Document {i+1}:\")\n",
    "#     print(doc.page_content)  # Print the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by type\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: str\n",
    "\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if isinstance(element, Table):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif isinstance(element, CompositeElement):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Separate into text and table elements\n",
    "table_elements = [e.text for e in categorized_elements if e.type == \"table\"]\n",
    "text_elements = [e.text for e in categorized_elements if e.type == \"text\"]\n",
    "\n",
    "print(f\"Extracted {len(table_elements)} tables and {len(text_elements)} text elements.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5090,
     "status": "ok",
     "timestamp": 1737778991231,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "uR5g_wVqkdDY"
   },
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "# import pytesseract\n",
    "\n",
    "# images = convert_from_path(\"/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/01_pdf_mysql_ai_agent/RAG_AGENT_FOR_QA_FROM_STRUCTURAL_UNSTRUCTURAL_DATA/gst_notebook/PDF_DATA/75183bos60698-p1-m1.pdf\")\n",
    "# for image in images:\n",
    "#     text = pytesseract.image_to_string(image)\n",
    "#     print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2156,
     "status": "ok",
     "timestamp": 1737779029227,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "BJMOLXtwiEok"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Directory to store ChromaDB data\n",
    "chroma_db_directory = \"./DATA/GST_DATA_VECTOR_DB\"\n",
    "\n",
    "# # Initialize ChromaDB with persistence\n",
    "# embedding_function = OpenAIEmbeddings()\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"pdf_data\",\n",
    "#     persist_directory=chroma_db_directory,  # Directory for persistent storage\n",
    "#     embedding_function=embedding_function\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1737779034697,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "gmrknIMAiPWV"
   },
   "outputs": [],
   "source": [
    "# Store text elements\n",
    "doc_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "text_documents = [\n",
    "    Document(page_content=text, metadata={\"type\": \"text\", \"doc_id\": doc_id})\n",
    "    for text, doc_id in zip(text_elements, doc_ids)\n",
    "]\n",
    "\n",
    "# Store table elements\n",
    "table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "table_documents = [\n",
    "    Document(page_content=table, metadata={\"type\": \"table\", \"doc_id\": table_id})\n",
    "    for table, table_id in zip(table_elements, table_ids)\n",
    "]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vectorstore.add_documents(text_documents)\n",
    "\n",
    "if table_documents:  # Check if table_documents is not empty\n",
    "    vectorstore.add_documents(table_documents)\n",
    "\n",
    "# Persist the vector store to disk\n",
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1737779039494,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "Xacnz6YYiYRu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18780/3860787160.py:16: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Reload the persisted vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"pdf_data\",\n",
    "    persist_directory=chroma_db_directory,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a QA chain with the retriever\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "# Query the stored data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1737779135040,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "2jsAjUePnZTQ",
    "outputId": "53449cca-0235-401a-a798-7c42a1166af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'query': 'Net profit for the year 20X1` 18,00,000 Net profit for the year 20X2` 60,00,000 No. of equity shares outstanding until 30th September 20X2  20,00,000, Bonus issue 1st October 20X2 was two equity shares for each equity share outstanding at 30th September, 20X2 Calculate Basic Earnings Per Share', 'result': 'The Basic Earnings Per Share (EPS) for the year 20X1 is calculated by dividing the net profit for the year by the number of equity shares outstanding. \\n\\nFor 20X1, the EPS would be `18,00,000 / 20,00,000 = `0.90 per share.\\n\\nFor 20X2, the calculation is a bit more complex due to the bonus issue. The bonus issue resulted in an additional 20,00,000 x 2 = 40,00,000 shares. So, the total number of shares for 20X2 would be 20,00,000 (original shares) + 40,00,000 (bonus shares) = 60,00,000 shares.\\n\\nTherefore, the EPS for 20X2 would be `60,00,000 / 60,00,000 = `1.00 per share.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the summary of the document's text?\"\n",
    "query = \"What is the average tokens in prompt for Stanford SHP in one line answere?\"\n",
    "query = \"What is the average tokens in responses for Meta\"\n",
    "query = \"what is AS 23?\"\n",
    "user_input = \"Net profit for the year 20X1` 18,00,000 Net profit for the year 20X2` 60,00,000 No. of equity shares outstanding until 30th September 20X2  20,00,000, Bonus issue 1st October 20X2 was two equity shares for each equity share outstanding at 30th September, 20X2 Calculate Basic Earnings Per Share\"\n",
    "\n",
    "result = qa_chain.invoke(user_input)\n",
    "print(\"Answer:\", result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Net profit for the year 20X1` 18,00,000 Net profit for the year 20X2` 60,00,000 No. of equity shares outstanding until 30th September 20X2  20,00,000, Bonus issue 1st October 20X2 was two equity shares for each equity share outstanding at 30th September, 20X2 Calculate Basic Earnings Per Share',\n",
       " 'result': 'The Basic Earnings Per Share (EPS) for the year 20X1 is calculated by dividing the net profit for the year by the number of equity shares outstanding. \\n\\nFor 20X1, the EPS would be `18,00,000 / 20,00,000 = `0.90 per share.\\n\\nFor 20X2, the calculation is a bit more complex due to the bonus issue. The bonus issue resulted in an additional 20,00,000 x 2 = 40,00,000 shares. So, the total number of shares for 20X2 would be 20,00,000 (original shares) + 40,00,000 (bonus shares) = 60,00,000 shares.\\n\\nTherefore, the EPS for 20X2 would be `60,00,000 / 60,00,000 = `1.00 per share.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737779143996,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "UFJe6ezqptHq",
    "outputId": "aee3fedd-a414-446b-ff7b-502ad009896a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Basic Earnings Per Share (EPS) for the year 20X1 is calculated by dividing the net profit for the year by the number of equity shares outstanding. ',\n",
       " '',\n",
       " 'For 20X1, the EPS would be `18,00,000 / 20,00,000 = `0.90 per share.',\n",
       " '',\n",
       " 'For 20X2, the calculation is a bit more complex due to the bonus issue. The bonus issue resulted in an additional 20,00,000 x 2 = 40,00,000 shares. So, the total number of shares for 20X2 would be 20,00,000 (original shares) + 40,00,000 (bonus shares) = 60,00,000 shares.',\n",
       " '',\n",
       " 'Therefore, the EPS for 20X2 would be `60,00,000 / 60,00,000 = `1.00 per share.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result'].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1737778995250,
     "user": {
      "displayName": "Auditory Lab",
      "userId": "04585296647736496459"
     },
     "user_tz": -330
    },
    "id": "GTIgQInNp4t0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNnBZzuF7X9SuR+LM+mq8m3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
