{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import create_model\n",
    "import inspect,json\n",
    "from inspect import Parameter\n",
    "\n",
    "print(load_dotenv())\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/v0.2/docs/integrations/chat/huggingface/\n",
    "\n",
    "os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\n",
    "    \"Enter your Hugging Face API key: \"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/botenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/brainwired/D/BW_ML/01_AUG_FARM_TEST/study/botenv/lib/python3.12/site-packages/langchain_huggingface/chat_models/huggingface.py:130: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.llms.huggingface_text_gen_inference import (  # type: ignore[import-not-found]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\", #llm model\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0,api_key=\"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\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonschema(f):\n",
    "    \"\"\"\n",
    "    Generate a JSON schema for the input parameters of the given function.\n",
    "\n",
    "    Parameters:\n",
    "        f (FunctionType): The function for which to generate the JSON schema.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the function name, description, and parameters schema.\n",
    "    \"\"\"\n",
    "    kw = {n: (o.annotation, ... if o.default == Parameter.empty else o.default)\n",
    "            for n, o in inspect.signature(f).parameters.items()}\n",
    "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
    "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11886/3911407637.py:13: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "def abc(num1:int,num2:int)->int:\n",
    "    \"compute the abc bewteen the two numbers\"\n",
    "    return 2*num1*num2\n",
    "abc(2,3)\n",
    "\n",
    "abc_schema = jsonschema(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask huggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to a popular ethics thought experiment called the \"paradox of the unstoppable force and the immovable object,\" when an unstoppable force meets an immovable object, there is no logical result because the definitions of the two concepts are inherently contradictory. The phrase \"unstoppable force\" implies that the force will always continue moving, while \"immovable object\" suggests that the object cannot be moved. If both meet each other\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='NETWORKDIaNET toms tomodderware to simulated to communicate virtual machines to create for to to simulate to test to for to replace to for to to allow to create to improveware to\\n to execute to to to test to simulate to sim to create to simulate to create, and to show to simulate, and to sim to simulate, and to create to simulate, and to simulate, and to create to simulate, tosim to to simulate to and to simulate, and to to to to sim to to to to simulate to sim to to to simulate, and to simulate, and to simulate to simulate, and to simulate, and to simulate to simulate to and to virtual to simulate and to simulate to sim to and simulate, and to simulate to simulate, to to to simulate, and to simulate, and to simulate, and to simulate to simulate, and to simulate, and to simulate, and to simulate to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and to simulate, and', additional_kwargs={}, response_metadata={}, id='run-46c47a8b-7759-4220-9949-bdbee6a47c20-0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ai_msg.content)\n",
    "await chat_model.ainvoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add the custom function to model and get work done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"cal sum the abc between the 2 and 3\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m sorry but your input is not clear. Please provide more context or information.\\n\\nWithout further details, I\\'m assuming that \"cal sum\" means calculate the sum, \"abc\" refers to a three-letter group, and the numbers 2 and 3 indicate limited ranges.\\n\\nIf that\\'s the case, please let me know which three letters you\\'re referring to, and I\\'ll calculate their sum between 2 and 3. Otherwise, please', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=100, prompt_tokens=52, total_tokens=152), 'model': '', 'finish_reason': 'length'}, id='run-88056623-5fbe-43c6-8870-27c1e819b6a2-0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above the oupput is asking proper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the custom function to model \n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant i passigng python function please perform task smartly\"),\n",
    "    HumanMessage(\n",
    "        content=\"compute abc between the 2 and 3\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chat_model = ChatHuggingFace(llm=custom_llm\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"If you're referring to the arithmetic mean (average), you can calculate ABC between 2 and 3 as follows:\\n\\n1. Add the two numbers: 2 + 3 = 5\\n2. Divide the sum by 2 (number of terms): ABC = 5 / 2 = 2.5\\n\\nTherefore, the average (ABC) of 2 and 3 is 2.5.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=96, prompt_tokens=61, total_tokens=157), 'model': '', 'finish_reason': 'stop'}, id='run-6f37d8e9-f05c-404f-9ecf-540619e8048c-0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_tools = custom_chat_model.bind_tools([abc_schema])\n",
    "ai_msg = chat_with_tools.invoke(messages)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments={'num1': 2, 'num2': 3}, name='abc', description=None), id='0', type='function')]}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=41, prompt_tokens=261, total_tokens=302), 'model': '', 'finish_reason': 'stop'}, id='run-d34d5c2e-a486-4968-ab51-cad0ef38e47e-0', tool_calls=[{'name': 'abc', 'args': {'num1': 2, 'num2': 3}, 'id': '0', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tool using langchian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def abc(a: int, b: int) -> int:\n",
    "   \"\"\"Multiply two numbers.\"\"\"\n",
    "   return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.description\n",
    "\n",
    "abc.invoke({\"a\": 2, \"b\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools =[abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool = custom_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chat_with_tool = ChatHuggingFace(llm=llm_with_tool\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta', huggingfacehub_api_token='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', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation'), tokenizer=LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, model_id='HuggingFaceH4/zephyr-7b-beta'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'abc', 'description': 'Compute abc between two numbers', 'parameters': {'properties': {'num1': {'type': 'integer'}, 'num2': {'type': 'integer'}}, 'required': ['num1', 'num2'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments={'num1': 2, 'num2': 3}, name='abc', description=None), id='0', type='function')]}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=39, prompt_tokens=231, total_tokens=270), 'model': '', 'finish_reason': 'stop'}, id='run-c13f4286-6318-4617-b02c-6543c615d159-0', tool_calls=[{'name': 'abc', 'args': {'num1': 2, 'num2': 3}, 'id': '0', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools.invoke(\"compute abc bewtween 2,3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant i passigng python function please perform task smartly with\"),\n",
    "    HumanMessage(\n",
    "        content=\"compute abc between the 2 and 3\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments={'num1': 2, 'num2': 3}, name='abc', description=None), id='0', type='function')]}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=41, prompt_tokens=231, total_tokens=272), 'model': '', 'finish_reason': 'stop'}, id='run-e2455fd5-11f1-46e3-aaac-eb9a3123bc4c-0', tool_calls=[{'name': 'abc', 'args': {'num1': 2, 'num2': 3}, 'id': '0', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools.invoke('compute abc between the 2 and 3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
